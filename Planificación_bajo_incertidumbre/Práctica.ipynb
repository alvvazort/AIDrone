{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica de planificación bajo incertidumbre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inteligencia Artificial\n",
    "### Grado en Ingeniería Informática - Ingeniería del Software\n",
    "### Universidad de Sevilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paquete de _Python_ [pymdptoolbox](https://github.com/sawcordwell/pymdptoolbox) proporciona un marco de trabajo para procesos de decisión de Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paquete proporciona tres módulos:\n",
    "* El módulo _mdp_ es el que implementa los procesos de decisión de Markov y los algoritmos de iteración de valores y de políticas, entre otros.\n",
    "* El módulo _util_ proporciona algunas funciones para comprobar la correcta descripción del proceso de decisión de Markov, como por ejemplo que la función de transición de cada acción aplicada a cada estado es una distribución de probabilidad (las probabilidades de los nuevos estados están en el intervalo $[0, 1]$ y suman $1$).\n",
    "* El modulo _example_ proporciona algunos ejemplos.\n",
    "\n",
    "En esta práctica solo usaremos el primero de esos módulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox.mdp as mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos para calcular una política óptima utilizan operaciones matriciales, por lo que también necesitaremos hacer uso del paquete [Numpy](https://numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo del robot y las cinco localizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que en el tema hemos visto un ejemplo de proceso de decisión de Markov en el que tenemos un robot que puede encontrarse en una de entre cinco localizaciones y trata de moverse entre ellas, con el objetivo de llegar a una localización concreta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los estados del sistema están representados por un predicado `en`, que indica la localización en la que se encuentra el robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "estados = ['en(l1)', 'en(l2)', 'en(l3)', 'en(l4)', 'en(l5)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las posibles acciones que puede realizar el robot son `esperar` o `ir` de una localización a otra (pero no para cualquier par de localizaciones hay un camino directo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "acciones = ['esperar',\n",
    "            'ir(l1, l2)', 'ir(l1, l4)',\n",
    "            'ir(l2, l1)', 'ir(l2, l3)',\n",
    "            'ir(l3, l2)', 'ir(l3, l4)',\n",
    "            'ir(l4, l1)', 'ir(l4, l3)', 'ir(l4, l5)',\n",
    "            'ir(l5, l2)', 'ir(l5, l4)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad, las listas de estados y acciones anteriores no son necesarias, ya que el paquete trabaja con la enumeración de estados (`0` es el estado `en(l1)`, `1` es el estado `en(l2)`, ...) y de las acciones (`0` es la acción `esperar`, `1` es la acción `ir(l1, l2)`, ...). Pero nos servirán para transformar las respuestas de los algoritmos a un formato más amigable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acción `esperar` deja al robot en la misma localización en que se encuentre con probabilidad $1$ y tiene coste $0$. La función de transición del resto de acciones, sus costes y las recompensas de los estados se encuentran recogidas en el siguiente gráfico:\n",
    "\n",
    "![](Recompensas_y_costes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las recompensas de los estados se representan mediante un array unidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0  100 -100]\n"
     ]
    }
   ],
   "source": [
    "recompensas_estados = numpy.array([0, 0, 0, 100, -100])\n",
    "print(recompensas_estados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de transición y el coste de aplicación para cada acción se representan, respectivamente, mediante un array bidimensional y un array unidimensional. Por ejemplo, para la acción `esperar` la función de transición vendrá dada por la matriz identidad $5 \\times 5$ y el coste de aplicación por el vector nulo de longitud $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "transición_esperar = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                  [0, 1, 0, 0, 0],\n",
    "                                  [0, 0, 1, 0, 0],\n",
    "                                  [0, 0, 0, 1, 0],\n",
    "                                  [0, 0, 0, 0, 1]])\n",
    "print(transición_esperar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "coste_esperar = numpy.array([0, 0, 0, 0, 0])\n",
    "print(coste_esperar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un detalle importante es que el paquete asume que todas las acciones son ejecutables en todos los estados. Representaremos que una acción no es ejecutable en un estado estableciendo que el estado no cambia y que la ejecución tiene un coste muy alto (de hecho, podremos establecer coste infinito). De esta forma, el algoritmo no seleccionará la acción para la política óptima. Por ejemplo, para la acción `ir(l1, l2)` se tiene la siguiente matriz de transición y vector de costes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "transición_ir_l1_l2 = numpy.array([[0, 1, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "print(transición_ir_l1_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.  inf  inf  inf  inf]\n"
     ]
    }
   ],
   "source": [
    "coste_ir_l1_l2 = numpy.array([100, numpy.inf, numpy.inf, numpy.inf, numpy.inf])\n",
    "print(coste_ir_l1_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análogamente, para el resto de acciones se tienen las siguientes matrices de transición y vectores de coste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transición_ir_l1_l4 = numpy.array([[.5, 0, 0, .5, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l1_l4 = numpy.array([1, numpy.inf, numpy.inf, numpy.inf, numpy.inf])\n",
    "\n",
    "transición_ir_l2_l1 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [1, 0, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l2_l1 = numpy.array([numpy.inf, 100, numpy.inf, numpy.inf, numpy.inf])\n",
    "\n",
    "transición_ir_l2_l3 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 0, .8, 0, .2],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l2_l3 = numpy.array([numpy.inf, 1, numpy.inf, numpy.inf, numpy.inf])\n",
    "\n",
    "transición_ir_l3_l2 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l3_l2 = numpy.array([numpy.inf, numpy.inf, 1, numpy.inf, numpy.inf])\n",
    "\n",
    "transición_ir_l3_l4 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l3_l4 = numpy.array([numpy.inf, numpy.inf, 100, numpy.inf, numpy.inf])\n",
    "\n",
    "transición_ir_l4_l1 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [1, 0, 0, 0, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l4_l1 = numpy.array([numpy.inf, numpy.inf, numpy.inf, 1, numpy.inf])\n",
    "\n",
    "transición_ir_l4_l3 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l4_l3 = numpy.array([numpy.inf, numpy.inf, numpy.inf, 100, numpy.inf])\n",
    "\n",
    "transición_ir_l4_l5 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 0, 1],\n",
    "                                   [0, 0, 0, 0, 1]])\n",
    "coste_ir_l4_l5 = numpy.array([numpy.inf, numpy.inf, numpy.inf, 100, numpy.inf])\n",
    "\n",
    "transición_ir_l5_l2 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 1, 0, 0, 0]])\n",
    "coste_ir_l5_l2 = numpy.array([numpy.inf, numpy.inf, numpy.inf, numpy.inf, 1])\n",
    "\n",
    "transición_ir_l5_l4 = numpy.array([[1, 0, 0, 0, 0],\n",
    "                                   [0, 1, 0, 0, 0],\n",
    "                                   [0, 0, 1, 0, 0],\n",
    "                                   [0, 0, 0, 1, 0],\n",
    "                                   [0, 0, 0, 1, 0]])\n",
    "coste_ir_l5_l4 = numpy.array([numpy.inf, numpy.inf, numpy.inf, numpy.inf, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para calcular una política óptima del proceso de decisión de Markov hay que pasarle a los algoritmos un array tridimensional de todas las matrices de transición y un array bidimensional de las recompensas de los estados menos los costes de las acciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transiciones_sistema = numpy.array([transición_esperar,\n",
    "                                    transición_ir_l1_l2,\n",
    "                                    transición_ir_l1_l4,\n",
    "                                    transición_ir_l2_l1,\n",
    "                                    transición_ir_l2_l3,\n",
    "                                    transición_ir_l3_l2,\n",
    "                                    transición_ir_l3_l4,\n",
    "                                    transición_ir_l4_l1,\n",
    "                                    transición_ir_l4_l3,\n",
    "                                    transición_ir_l4_l5,\n",
    "                                    transición_ir_l5_l2,\n",
    "                                    transición_ir_l5_l4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0. -100.   -1.  -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf]\n",
      " [   0.  -inf  -inf -100.   -1.  -inf  -inf  -inf  -inf  -inf  -inf  -inf]\n",
      " [   0.  -inf  -inf  -inf  -inf   -1. -100.  -inf  -inf  -inf  -inf  -inf]\n",
      " [ 100.  -inf  -inf  -inf  -inf  -inf  -inf   99.    0.    0.  -inf  -inf]\n",
      " [-100.  -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf -101. -200.]]\n"
     ]
    }
   ],
   "source": [
    "# Transformamos el vector de recompensas en una matriz 5x1\n",
    "matriz_recompensas = recompensas_estados.reshape(5, 1)\n",
    "# Creamos una matriz donde cada columna es el vector de costes de una acción\n",
    "matriz_costes = numpy.column_stack([coste_esperar,\n",
    "                                    coste_ir_l1_l2,\n",
    "                                    coste_ir_l1_l4,\n",
    "                                    coste_ir_l2_l1,\n",
    "                                    coste_ir_l2_l3,\n",
    "                                    coste_ir_l3_l2,\n",
    "                                    coste_ir_l3_l4,\n",
    "                                    coste_ir_l4_l1,\n",
    "                                    coste_ir_l4_l3,\n",
    "                                    coste_ir_l4_l5,\n",
    "                                    coste_ir_l5_l2,\n",
    "                                    coste_ir_l5_l4])\n",
    "\n",
    "recompensas_sistema = matriz_recompensas - matriz_costes\n",
    "print(recompensas_sistema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener una política óptima mediante el algoritmo de iteración de valores hay que crear una instancia adecuada de la clase `ValueIteration`. Hay que tener en cuenta que el valor de $\\epsilon$ que se proporciona es la diferencia máxima que se requiere entre el último $U_{n}$ calculado y $U^{*}$, para lo que se deriva a partir de él la diferencia máxima adecuada entre $U_{n}$ y $U_{n - 1}$. Por otra parte, $U_{0}$ asigna utilidad inicial $0$ a todos los estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo_robot_VI = mdp.ValueIteration(\n",
    "    transitions=transiciones_sistema,\n",
    "    reward=recompensas_sistema,\n",
    "    discount=0.9,\n",
    "    epsilon=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar el algoritmo basta usar el método `run` de la clase anterior, pero antes establecemos el modo verboso, para que proporcione detalles de los cálculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tV-variation\n",
      "    1\t\t  200.0\n",
      "    2\t\t  91.0\n",
      "    3\t\t  81.0\n",
      "    4\t\t  28.0\n",
      "    5\t\t  4.191750000000042\n",
      "    6\t\t  1.8862875000000372\n",
      "    7\t\t  0.8488293750000651\n",
      "    8\t\t  0.38197321875003354\n",
      "    9\t\t  0.17188794843758615\n",
      "    10\t\t  0.07734957679679155\n",
      "    11\t\t  0.034807309558630095\n",
      "    12\t\t  0.015663289301357963\n",
      "    13\t\t  0.0070484801857446655\n",
      "Iterating stopped, epsilon-optimal policy found.\n"
     ]
    }
   ],
   "source": [
    "ejemplo_robot_VI.setVerbose()\n",
    "ejemplo_robot_VI.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política óptima se encuentra guardada en el atributo `policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 6, 0, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo_robot_VI.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la siguiente expresión podemos verla de manera más amigable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el estado en(l1) ejecuta la acción ir(l1, l4)\n",
      "En el estado en(l2) ejecuta la acción ir(l2, l3)\n",
      "En el estado en(l3) ejecuta la acción ir(l3, l4)\n",
      "En el estado en(l4) ejecuta la acción esperar\n",
      "En el estado en(l5) ejecuta la acción ir(l5, l4)\n"
     ]
    }
   ],
   "source": [
    "for estado, i in zip(estados, ejemplo_robot_VI.policy):\n",
    "    print(f'En el estado {estado} ejecuta la acción {acciones[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera análoga, se puede obtener una política óptima mediante el algoritmo de iteración de políticas creando una instancia adecuada de la clase `PolicyIteration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tNumber of different actions\n",
      "    1\t\t  3\n",
      "    2\t\t  1\n",
      "    3\t\t  0\n",
      "Iterating stopped, unchanging policy found.\n",
      "\n",
      "En el estado en(l1) ejecuta la acción ir(l1, l4)\n",
      "En el estado en(l2) ejecuta la acción ir(l2, l3)\n",
      "En el estado en(l3) ejecuta la acción ir(l3, l4)\n",
      "En el estado en(l4) ejecuta la acción esperar\n",
      "En el estado en(l5) ejecuta la acción ir(l5, l4)\n"
     ]
    }
   ],
   "source": [
    "ejemplo_robot_PI = mdp.PolicyIteration(\n",
    "    transitions=transiciones_sistema,\n",
    "    reward=recompensas_sistema,\n",
    "    discount=0.9,\n",
    "    policy0=numpy.array([0, 0, 0, 0, 0])  # La política inicial es esperar en cada estado\n",
    ")\n",
    "\n",
    "ejemplo_robot_PI.setVerbose()\n",
    "ejemplo_robot_PI.run()\n",
    "\n",
    "print()\n",
    "for estado, i in zip(estados, ejemplo_robot_PI.policy):\n",
    "    print(f'En el estado {estado} ejecuta la acción {acciones[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de la piscifactoría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos a cargo de una piscifactoría. Cada temporada hay que decidir qué parte de la población de peces se captura y comercializa y qué parte se deja en la piscifactoría para que se reproduzcan. Si denotamos $x$ la cantidad de peces que hay en la piscifactoría al inicio de la temporada, la recompensa que se obtiene al comercializar $y < x$ peces es $10y$. Si $z = x - y$ es la cantidad de peces que quedan, entonces para la siguiente temporada puede haber ocurrido uno de los siguientes casos:\n",
    "* Con probabilidad $0.2$, la temporada ha sido buena y los peces se han reproducido hasta alcanzar la cantidad de $1.8z$.\n",
    "* Con probabilidad $0.7$, la temporada ha sido normal y los peces se han reproducido hasta alcanzar la cantidad de $1.4z$.\n",
    "* Con probabilidad $0.1$, la temporada ha sido mala y los peces no se han reproducido lo suficiente, por lo que queda una cantidad de $0.9z$.\n",
    "\n",
    "En todos los casos se redondea hacia arriba y hay que tener también en cuenta que la piscifactoría tiene una capacidad máxima de $N$ peces. También puede ocurrir que coincidan alguno, o incluso todos, los valores anteriores, en cuyo caso las probabilidades correspondientes se suman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que $N = 10$. Entonces:\n",
    "* Los posibles estados son: $0, 1, \\dotsc, 10$.\n",
    "* La recompensa de cada estado es $0$, ya que lo que se pretende es maximizar los beneficios de comercializar los peces, más que disponer de una cierta cantidad de peces en la piscifactoría (una variante a considerar sería establecer recompensas menores para los estados con menos peces, para así evitar las políticas que dejen pocos peces en la piscifactoría).\n",
    "* Las posibles acciones son comercializar $y$ peces, para cada $y = 0, 1, \\dotsc, 10$.\n",
    "* El coste de comercializar $y$ peces es $-10y$. Obsérvese que son valores negativos para considerarlos como beneficio de aplicar la acción.\n",
    "* Sin embargo, si en la piscifactoría hay $x$ peces, entonces el coste de comercializar $y > 0$ peces, con $y \\geq x$, es $\\infty$, ya que esas acciones no serían aplicables. Comercializar $0$ peces siempre es aplicable.\n",
    "* Si en la piscifactoría hay $x = 8$ peces y comercializamos $y = 2$ peces, entonces la función de transición para esa acción sería:\n",
    "  * Con probabilidad 0.2 pasaríamos al estado $1.8 (8 - 2) = 10$ (ya que esa es la capacidad máxima de la factoría).\n",
    "  * Con probabilidad 0.7 pasaríamos al estado $1.4 (8 - 2) = 9$.\n",
    "  * Con probabilidad 0.1 pasaríamos al estado $0.9 (8 - 2) = 6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1**: Definir una función que, dadas la capacidad $N$ de la piscifactoria y la cantidad $y$ de peces a comercializar, devuelva el vector de costes de esa acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**: Definir una función que, dadas la capacidad $N$ de la piscifactoria y la cantidad $y$ de peces a comercializar, devuelva la matriz de transición de esa acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**: Definir una función que, dada la capacidad $N$ de la piscifactoría, devuelva la matriz de recompensas del proceso de decisión de Markov correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4**: Definir una función que, dada la capacidad $N$ de la piscifactoría, devuelva el array de matrices de transición del proceso de decisión de Markov correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5**: Aplicar los algoritmos de iteración de valores y políticas para calcular políticas óptimas considerando distintos valores de $N$ (por ejemplo, distintas potencias de $10$) y del factor de descuento (por ejemplo, $\\gamma = 0.1, 0.5, 0.9$, para tener poco, algo o mucho en cuenta lo que pueda ocurrir en el futuro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
